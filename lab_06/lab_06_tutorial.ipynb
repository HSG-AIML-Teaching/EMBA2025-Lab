{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajNMrbY1MUml"
      },
      "source": [
        "# **Using ChatGPT with Python**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Securely Handling Sensitive Data in Colab**"
      ],
      "metadata": {
        "id": "qgLQUPnAXPDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use OpenAI's GPT models the API key needs to be set up.\n",
        "\n",
        "Before being able to use the Chat Completions API\n",
        "endpoint we need to authenticate ourselves using the **API key**. Having finished the preparatory assignments you should have your own API key which you can use during the course as well as the post assignment.\n",
        "\n",
        "When working with sensitive information like API keys or passwords in Google Colab, it's crucial to handle data securely. Two common approaches for this are using **Colab's Secrets Manager**, which stores and retrieves secrets without exposing them in the notebook, and `getpass`, a Python function that securely prompts users to input secrets during runtime without showing them. Both methods help ensure your sensitive data remains protected."
      ],
      "metadata": {
        "id": "U7B32sA1XDs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Option 1: Using Google Colab Secrets Manager**"
      ],
      "metadata": {
        "id": "aiuHoVgAW58K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vILGOv5Dau3Q"
      },
      "source": [
        "Google Colab provides an integrated Secrets Manager, allowing you to securely store and retrieve sensitive information such as API keys or authentication tokens without hardcoding them in your notebook.\n",
        "\n",
        "**Step 1: Store Your Secret in Colab**\n",
        "\n",
        "1.   In the Colab notebook, navigate to the left sidebar.\n",
        "2.   Click on the **“Secrets”** tab (represented by a key icon).\n",
        "3. Add your secret by clicking on **“+ Add a new secret”**. For example, you might add a secret called `OPENAI_API_KEY` with the value of your API key.\n",
        "\n",
        "**Step 2: Access the Secret in Your Notebook**\n",
        "\n",
        "Once you've added a secret, you can easily access it from within the notebook.\n",
        "\n",
        "`OPENAI_API_KEY` is the name of the secret you've added in the Colab Secrets Manager. It will be retrieved securely without having to expose the key in the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D6e8iVJ-L9r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Option 2: Using Python's `getpass` for Secret Input**"
      ],
      "metadata": {
        "id": "ApcrD_OGX7Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, the `getpass` module allows you to securely input secrets (e.g., passwords or API keys) during runtime, making sure they're not visible in the notebook output.\n",
        "\n",
        "Here, the `getpass.getpass` function prompts the user to enter the secret without displaying it as they type, ensuring that sensitive data isn't exposed."
      ],
      "metadata": {
        "id": "mVUeKxs_WzXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass()"
      ],
      "metadata": {
        "id": "M_fgOXZ7W0xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Installing OpenAI**"
      ],
      "metadata": {
        "id": "5iqfA8ZvYJBN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50s3eLZ7xUnl"
      },
      "source": [
        "In this notebook you will learn how to use ChatGPT with Python for various tasks such as **text generation**, **speech-to-text**,  **text-to-speech**, and  **image generation**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUayPEw170rz"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48SSfF_F8ADV"
      },
      "source": [
        "## **Text Generation**\n",
        "\n",
        "OpenAI's text generation **models** (often called generative pre-trained transformers or large language models) have been trained to understand natural language, code, and images. The latest models, `gpt-4o` (large model), `gpt-4o-mini` (small model), and the `o1`  familty of models (reasoning model) are accessed through the **chat completions API endpoint**.\n",
        "\n",
        "> For an overview of the different models for text generation you can visit https://platform.openai.com/docs/models.\n",
        "\n",
        "The models provide text outputs in response to  inputs given to them. The inputs to these models are also referred to as **\"prompts\"**. Designing a prompt is essentially how you “program” a large language model, usually by providing instructions or some examples of how to successfully complete a task.\n",
        "\n",
        "To use one of these models via the OpenAI API, you can send a request containing the inputs and your API key, and receive a response containing the model's output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SartI438kJ7Y"
      },
      "source": [
        "\n",
        "### **Chat Completions API**\n",
        "\n",
        "The Chat Completions API allows you to interact with chat models by providing a list of messages as input. The model processes the conversation history and returns a message generated based on the input, enabling dynamic and context-aware conversations.\n",
        "\n",
        "To use the Chat Completions API, you first need to import the necessary class and initialize the OpenAI client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCNMr6_cm4YC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5cipNaSngJa"
      },
      "source": [
        "After successful authentication the Chat Completions API can be used. The API call has two required inputs:\n",
        "\n",
        "* `model`: the name of the model you want to use (e.g., `gpt-4o`, `gpt-4o-mini`)\n",
        "* `messages`: a list of message objects, where each object has two required fields:\n",
        "   * `role`: the role of the messenger (either developer, user, or assistant)\n",
        "   * `content`: the content of the message (e.g., Write me a beautiful poem)\n",
        "\n",
        "There are additional optional parameters:\n",
        "\n",
        "* `temperature`: the temperature parameter allows you to balance randomness and determinism in the output. Low temperature values make the output more focussed and deterministic, while a high temperature increases randomness in the output and produce more unexpected outputs.\n",
        "\n",
        "Typically, a conversation will start with a system message that tells the assistant how to behave, followed by alternating user and assistant messages, but you are not required to follow this format.\n",
        "\n",
        "While the **developer message** helps set the behavior of the assistant, the **user messages** provide requests or comments for the assistant to respond to, and  **assistant messages** store previous assistant responses, but can also be written by you to give examples of desired behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09QaI2PqzQbs"
      },
      "source": [
        "An example Chat Completions API call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDQG-s4EzZmM"
      },
      "outputs": [],
      "source": [
        "#Example with a system message\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Explain generative AI to a 6 year old.\"},\n",
        "\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "bOY1opQwSJeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhhiirKMzcyP"
      },
      "source": [
        "The response format of a Chat Completion API response:\n",
        "\n",
        "* `id:` the ID of the request\n",
        "* `object: `the type of object returned (e.g., chat.completion)\n",
        "* `created:` the timestamp of the request\n",
        "* `model:` the full name of the model used to generate the response\n",
        "* `usage:` the number of tokens used to generate the replies, counting prompt, completion, and total\n",
        "* `choices: `a list of completion objects (only one, unless you set n greater than 1)\n",
        " * `message:` the message object generated by the model, with `role` and `content`\n",
        " * `finish_reason:` the reason the model stopped generating text (either stop, or length if max_tokens limit was reached)\n",
        " * `index: `the index of the specific completion in the list of the `choices` array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slptaVyu1lDu"
      },
      "source": [
        "We can extract the reply as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmcoFXzmzcdg"
      },
      "outputs": [],
      "source": [
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can make OpenAI output (such as ChatGPT responses) more readable in a Jupyter Notebook by displaying it as formatted Markdown using IPython's `display` functions.\n"
      ],
      "metadata": {
        "id": "Dhitn7qGHeQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "rp_vQ8s_FkAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response.choices[0].message.content))"
      ],
      "metadata": {
        "id": "qDLsCcaTHj-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXZ9yATI1wmz"
      },
      "source": [
        "### **Some Prompting Tipps**\n",
        "\n",
        "The next section provides some prompting tipps. For more details you can visit [here](https://platform.openai.com/docs/guides/prompt-engineering)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9p6YUWm0v_Z"
      },
      "source": [
        "#### **Developer Messages**\n",
        "Using developer messages you can provide context to a language model. This is essential because it helps guide the model's responses to align with the intended purpose or task. Here's why context matters:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Define the Model's Role:** The role the model plays is crucial in shaping its responses. Providing clear instructions about the model's purpose ensures that it stays on track with the task at hand. For example, when instructing the model to act as a professional translator, the message gives the model a clear sense of the expected response type (i.e., translation).\n",
        "\n",
        "  *Example: \"You are a professional translator. Translate the following technical document from English to Spanish, maintaining the technical accuracy and tone of the original content.\"*\n",
        "\n",
        "* **Set Behavioral Guidelines:** Setting behavioral guidelines helps shape the tone, style, and structure of the model's response. If you want a concise and professional tone, the developer message will help the model recognize the importance of brevity and clarity.\n",
        "\n",
        "  *Example: \"Provide concise, professional responses in bullet points. Limit each bullet point to one sentence, and make sure each response starts with a key phrase or action.\"*\n",
        "\n",
        "* **Provide Background Context:** Background context is incredibly important when tackling more complex tasks. It ensures the model understands the framework and scope of the subject matter, leading to responses that are grounded in the right knowledge.\n",
        "\n",
        "  *Example: \"Explain technical topics in simple terms for a beginner audience. Assume the reader has no prior knowledge of programming concepts. Provide analogies where necessary.\"*\n",
        "\n",
        "* **Ensure Task-Specific Focus:** Keeping the model focused on a specific task avoids unnecessary tangents. By framing the interaction carefully, you help the model narrow the scope to what's relevant to the user.\n",
        "\n",
        "  *Example: \"You are a travel guide. Recommend 3 activities in Paris for a family with kids. Each activity should be family-friendly, affordable, and have a brief description.\"*\n",
        "\n",
        "* **Reduce Ambiguity:** Reducing ambiguity means avoiding vague or unclear instructions, which ensures that the model understands exactly what is required. This leads to responses that are focused and relevant.\n",
        "\n",
        "  *Example: \"Write a Python function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain each step. Ensure that your code handles edge cases like negative numbers and large inputs gracefully.\"*\n",
        "\n",
        "* **Define Output Format:** Specifying how the response should be structured helps the model to organize its answer in a way that is easily digestible and meets the user’s needs.\n",
        "\n",
        "  *Example: Specify the output format clearly for the model to follow. The response should be structured as follows:*\n",
        "  \n",
        "  1. *Introduction: Provide a concise overview of the topic in 2-3 sentences, focusing on the key point.*\n",
        "\n",
        "  2. *Main Body: Break the content down into 2-4 main sections, each with a clear heading. Each section should contain bullet points or short paragraphs (3-4 sentences per point).*\n",
        "  \n",
        "  3. *Conclusion: Wrap up the response with a short summary (1-2 sentences) highlighting the key takeaways. Ensure the tone is informative, to-the-point, and free of jargon. Keep the total response under 300 words.*\n"
      ],
      "metadata": {
        "id": "QQ0SrHeo_JYQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkXo-xroy0NF"
      },
      "outputs": [],
      "source": [
        "#The developer message (with the \"role\" set to \"developer\") defines the model’s behavior,\n",
        "# its role, and how it should approach the task. This message is responsible for setting the\n",
        "# tone and framework within which the model operates.\n",
        "\n",
        "#The user message (with the \"role\" set to \"user\") provides the specific task or question that\n",
        "#the user wants the model to address. This is the content that the model will work on and respond to.\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "    {\"role\": \"developer\", \"content\": \"You are a friendly and approachable computer science teacher, who explains complex topics in simple, child-friendly terms. Your goal is to make topics fun and easy to understand for young children.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Explain generative AI in a fun and simple way, using examples that a 6-year-old would understand. Avoid technical jargon and make sure it’s engaging with a relatable analogy or story.\"},\n",
        "  ]\n",
        ")\n",
        "\n",
        "display(Markdown(response.choices[0].message.content))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_mq42I14L3E"
      },
      "outputs": [],
      "source": [
        "#The developer message provides instructions to the model on how it should behave during the tast.\n",
        "#It sets the tone (\"humorous,\" \"lighthearted,\" \"playful\") and defines the task (\"write a fun, engaging article about hiking\").\n",
        "#It guides the model in terms of style (humorous, full of puns, relatable to beginners\n",
        "\n",
        "#The user message contains the actual request or task that the user wants the model to address. It tells the model what to do with the context provided by the developer message.\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "    {\"role\": \"developer\", \"content\": \"You are a humorous AI with a lighthearted and playful tone. Your task is to write a fun, engaging article about hiking that is entertaining, full of puns, and relatable for beginners.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you write a short, fun article explaining why hiking is great for beginners? The article should be lighthearted, easy to understand, and highlight key benefits like health, mental clarity, and simple steps to get started.\"},\n",
        "  ]\n",
        ")\n",
        "\n",
        "display(Markdown(response.choices[0].message.content))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rc5EogxoZnnb"
      },
      "outputs": [],
      "source": [
        "#Developer message\n",
        "#Role Definition: teacher addressing an executive MBA audience. This means the model needs to provide a professional, clear, and concise explanation suitable for business-oriented students with an advanced level of education.\n",
        "#Task Instructions: It outlines the specific tasks the model should cover:\n",
        "#-Explain ML APIs (e.g., Google Cloud Vision, ChatGPT).\n",
        "#-xplain open-source, off-the-shelf pre-trained models.\n",
        "#-Compare ML APIs and open-source models.\n",
        "#Tone & Focus: The message sets expectations for a clear, educational tone, appropriate for the context of a class aimed at business professionals.\n",
        "\n",
        "#User message\n",
        "#Task Request: The user message asks the model to explain the topics (ML APIs, open-source models, and their comparison) for the class\n",
        "#Clarification: It doesn't need to specify the format, tone, or approach since the developer message already set those.\n",
        "#The user message is mainly confirming the need for an explanation without adding additional constraints or details.\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "    {\"role\": \"developer\", \"content\": \"You are a teacher teaching executive MBA students. 1) Explain what ML APIs such as Google Cloud Vision or Chat GPT are 2) Explain what open-source, off-the shelf pre-trained models are. 3) Compare the usage of ML APIs versus open-source, off-the shelf pretrained models.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you provide an explanation of these topics for my class?\"},\n",
        "  ]\n",
        ")\n",
        "\n",
        "display(Markdown(response.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w4twSTyzO8_"
      },
      "outputs": [],
      "source": [
        "#Developer message\n",
        "#Defines the Role: it specifies that the model should act as a teacher teaching executive MBA students.\n",
        "#Sets the Context: it asks the model to explain ML APIs (like Google Cloud Vision or ChatGPT), open-source models, and to compare the two.\n",
        "#Guides Style & Structure: it provides guidelines on how to deliver the content, such as making the explanation clear, concise, and using real-world examples.\n",
        "\n",
        "#User message\n",
        "#Specifies the Task:  it asks the model to explain the topics for their class.\n",
        "#Details the Output Format: it asks for the explanation to be detailed, engaging, and suitable for an MBA-level audience.\n",
        "#Focuses on Specific Outcomes: it asks for business-relevant examples and mentions that the comparison should be made between ML APIs and open-source models.\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "    {\"role\": \"developer\", \"content\": \"You are a teacher explaining advanced topics to executive MBA students. Your explanation should be clear, concise, and use **real-world examples** where possible. 1) Explain what ML APIs (such as Google Cloud Vision or ChatGPT) are, including their use cases and how businesses leverage them. 2) Explain what open-source, off-the-shelf pre-trained models are, with an emphasis on how they differ from ML APIs. 3) Compare the usage of ML APIs versus open-source pre-trained models, highlighting the pros and cons of each in a business context.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you provide a detailed explanation of these topics for my class? Please make the explanation clear, engaging, and appropriate for an MBA-level audience. It would be helpful to include business-relevant examples and highlight the practical implications of using ML APIs vs. open-source models.\"},\n",
        "  ]\n",
        ")\n",
        "\n",
        "display(Markdown(response.choices[0].message.content))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nWaZASY05os"
      },
      "source": [
        "#### **Few-shot Prompting**\n",
        "\n",
        "**Few-shot prompting** is a technique where you provide a language model with a **small number of examples** (typically 2-5) within the prompt to demonstrate the desired format, style, or behavior for a specific task.\n",
        "\n",
        "In the given example, the model is shown several pairs of corporate jargon statements and their plain English translations. These examples help the model understand the task's context and expected response style without explicitly programming the logic. By seeing the pattern of input-output pairs, the model can generalize and generate appropriate responses for new inputs, such as translating \"This late pivot means we don't have time to boil the ocean for the client deliverable.\" into \"This last-minute change means we can't spend excessive time on the client's project\". Few-shot prompting leverages the model's ability to infer rules and structures from minimal examples, making it a powerful and flexible tool for a wide range of tasks.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SGDF_Uaz4k2"
      },
      "outputs": [],
      "source": [
        "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
        "# Example adapted from: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\": \"developer\", \"content\": \"You are a helpful assistant specializing in translating corporate jargon into simple, plain English.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
        "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
        "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPgGLoLN2Qgj"
      },
      "source": [
        "To help clarify that the example messages are not part of a real conversation, and shouldn't be referred back to by the model, you can try setting the `name` field of `system` messages to `example_user` and `example_assistant`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUXAW3Dm2P0-"
      },
      "outputs": [],
      "source": [
        "# The business jargon translation example, but with example names for the example messages\n",
        "# Example adapted from: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\": \"developer\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
        "        {\"role\": \"developer\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
        "        {\"role\": \"developer\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
        "        {\"role\": \"developer\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
        "        {\"role\": \"developer\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
        "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
        "    ],\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COycJ3lhbqt3"
      },
      "outputs": [],
      "source": [
        "# description: classify titles into categories\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\": \"developer\", \"content\": \"You are a helpful, pattern-following assistant that classifies titles into categories.\"},\n",
        "        {\"role\": \"developer\", \"name\": \"example_user\", \"content\": \"Paris auf der Saslong nicht zu biegen\"},\n",
        "        {\"role\": \"developer\", \"name\":\"example_assistant\", \"content\": \"Ski Alpin\"},\n",
        "        {\"role\": \"developer\", \"name\":\"example_user\", \"content\": \"Kobayashi geht in Führung\"},\n",
        "        {\"role\": \"developer\", \"name\":\"example_assistant\", \"content\": \"Skispringen\"},\n",
        "        {\"role\": \"developer\", \"name\": \"example_user\", \"content\": \"Hütter fährt in Abfahrt aufs Podest\"},\n",
        "        {\"role\": \"developer\", \"name\":\"example_assistant\", \"content\": \"Ski Alpin\"},\n",
        "        {\"role\": \"developer\", \"name\": \"example_user\", \"content\": \"Schweizer Freudentag: Flury brilliert vor Hählen\"},\n",
        "        {\"role\": \"developer\", \"name\":\"example_assistant\", \"content\": \"Ski Alpin\"},\n",
        "        {\"role\": \"developer\", \"name\": \"example_user\", \"content\": \"Arsenal unterliegt Tottenham im Derby\"},\n",
        "        {\"role\": \"developer\", \"name\":\"example_assistant\", \"content\": \"Fussball\"},\n",
        "        {\"role\": \"user\", \"content\": \"Seoanes Gladbach muss sich mit Remis begnügen\"},\n",
        "        #{\"role\": \"user\", \"content\": \"Superadler Kraft: Endlich einmal gelbe Weihnachten\"},\n",
        "\n",
        "    ],\n",
        "\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPh9-whtnxMM"
      },
      "source": [
        "### **Customizing Chat Completions for Targeted Tasks**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next section, we will explore how to **customize Chat Completions for targeted tasks**. This involves creating **custom functions** that use a static system message while dynamically passing the user message as a parameter. This approach allows us to maintain control over the context provided to the model while tailoring the input to specific needs.\n",
        "\n",
        "In Lab07, we will build on this concept by using LangChain's Prompt Templates to achieve similar functionality. Prompt Templates enable us to define reusable prompts with placeholders, effectively managing both static and dynamic elements. This enhances flexibility, modularity, and efficiency in prompt design."
      ],
      "metadata": {
        "id": "hDLt4WS_602Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRIFkT3Uww7Y"
      },
      "source": [
        "#### **Using Chat Completions for Text Translation**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next section provides an example of a custom function for text translation. In this example, the system message sets the context for the model, while the text to be translated is dynamically passed as a parameter."
      ],
      "metadata": {
        "id": "t2yhbGbg9_I0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlSiqZpvn0A4"
      },
      "outputs": [],
      "source": [
        "def translateFromEnglishToGerman(text):\n",
        "  response = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"developer\",\n",
        "      \"content\": \"You will be provided with a sentence in English, and your task is to translate it into German.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": text\n",
        "    }\n",
        "  ],\n",
        "  temperature=0.7,\n",
        "  )\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4-PKRbBoNgJ"
      },
      "outputs": [],
      "source": [
        "translatedText = translateFromEnglishToGerman(\"My name is Barbara. What is yours?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSXYuZZUo_SC"
      },
      "outputs": [],
      "source": [
        "print(translatedText)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9qDuCeFnGsz"
      },
      "source": [
        "#### **Using Chat Completions for Image Prompt Generation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AX8KmJOmCq5"
      },
      "outputs": [],
      "source": [
        "def createImagePrompt(text):\n",
        "  response = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "     {\n",
        "        \"role\": \"developer\",\n",
        "        \"content\": \"You are a skillful prompt generator that refines text for an image prompt\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": text},\n",
        "  ]\n",
        "  )\n",
        "  return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3XfFfSCrNwA"
      },
      "outputs": [],
      "source": [
        "imagePrompt = createImagePrompt(\"nice and cinematic mountain ranges\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(imagePrompt))"
      ],
      "metadata": {
        "id": "i7rG6uNi92zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWOx8VRM3BBn"
      },
      "source": [
        "## **Text to Speech Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3qwy2nIERKC"
      },
      "source": [
        "The Audio API provides a `speech` endpoint based on our TTS (text-to-speech) model. It comes with 6 built-in voices (`alloy`, `echo`, `fable`, `onyx`, `nova`, and `shimmer`) and can be used to:\n",
        "\n",
        "* Narrate a written blog post\n",
        "* Produce spoken audio in multiple languages\n",
        "* Give real time audio output using streaming\n",
        "\n",
        "To check out supported languages visit https://platform.openai.com/docs/guides/text-to-speech."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KGHU8qvFSgM"
      },
      "source": [
        "### **Audio Generation Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZVxPHejrZXX"
      },
      "outputs": [],
      "source": [
        "def generateTextToSpeech(text):\n",
        "  speech_file_path = \"speech.mp3\"\n",
        "  response = client.audio.speech.create(\n",
        "    model=\"tts-1\",\n",
        "    voice=\"alloy\",\n",
        "    input=text\n",
        "  )\n",
        "\n",
        "  response.stream_to_file(speech_file_path)\n",
        "  return speech_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EILjTImd3VQa"
      },
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "fileName=generateTextToSpeech(\"Today is a wonderful day to build something people love!\")\n",
        "ipd.Audio(filename=fileName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymJNZuYwsK6Y"
      },
      "source": [
        "In the next code snippet there is a translation step into the target language (e.g., German) before the audio output is generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_0Z7SvqoZ7W"
      },
      "outputs": [],
      "source": [
        "# Here we create a whole simple pipeline starting from translation and then speech generation\n",
        "\n",
        "translatedText = translateFromEnglishToGerman(\"My name is Barbara. What is yours?\")\n",
        "\n",
        "fileName=generateTextToSpeech(translatedText)\n",
        "ipd.Audio(filename=fileName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TBpd5iXBtWL"
      },
      "source": [
        "## **Speech to Text (Audio API)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCZ67sevB0lD"
      },
      "source": [
        "The Audio API provides two speech to text endpoints, `transcriptions` and `translations`, based on our state-of-the-art open source large-v2 Whisper model. They can be used to:\n",
        "\n",
        "* Transcribe audio into whatever language the audio is in.\n",
        "* Translate and transcribe the audio into english.\n",
        "\n",
        "File uploads are currently limited to 25 MB and the\n",
        "following input file types are supported: `mp3`, `mp4`, `mpeg`, `mpga`, `m4a`, `wav`, and `webm`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7DlsqqTCdDt"
      },
      "source": [
        "### **Transcription Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oml-3lwNt9-3"
      },
      "outputs": [],
      "source": [
        "def transcribeAudio(fileName):\n",
        "  audio_file = open(fileName, \"rb\")\n",
        "  transcript = client.audio.transcriptions.create(\n",
        "    model=\"whisper-1\",\n",
        "    file=audio_file,\n",
        "    response_format=\"text\"\n",
        "  )\n",
        "  return transcript"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will now be asked to upload an .mp3 file which will then be transcribed.\n",
        "\n",
        "To explore the transcription capabilities of the Audio API you can experiment with one of the news audio files available at https://www.srf.ch/audio/nachrichten."
      ],
      "metadata": {
        "id": "6FPkI4thknpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O Podcast.mp3 https://download-media.srf.ch/world/audio/4x4_Podcast_radio/2025/01/4x4_Podcast_radio_AUDI20250128_NR_0038_3c81668cc4664fff9a26020d4eb47f0a.mp3?d=ap&assetId=e0ed83a1-efc1-373c-b4aa-9878c79f2e31\n"
      ],
      "metadata": {
        "id": "1NP0UZE4MbMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTBllScAvvHZ"
      },
      "outputs": [],
      "source": [
        "# The file name which is passed as a parameter to the function transcribeAudio needs to correspond to the file you have uploaded\n",
        "transcribedAudio= transcribeAudio(\"Podcast.mp3\")\n",
        "display(Markdown(transcribedAudio))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a bullet list with the news items covered."
      ],
      "metadata": {
        "id": "E3lM5-IwqS9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "    {\"role\": \"developer\", \"content\": \"You are a helpful news summarizer. Create a bulletlist of the different news topics covered with 1-2 sentences per item in German.\"},\n",
        "    {\"role\": \"user\", \"content\": \"You are provided with a transcript of a news Podcast.\" + transcribedAudio},\n",
        "  ]\n",
        ")\n",
        "\n",
        "summary = response.choices[0].message.content\n",
        "display(Markdown(summary))"
      ],
      "metadata": {
        "id": "wsTyacFVp5Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use a prompt to create a summary of the news podcast just focussing on topics we are intersted in."
      ],
      "metadata": {
        "id": "86BdzYjDmsdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-4o\",\n",
        "  messages=[\n",
        "    {\"role\": \"developer\", \"content\": \"You are a helpful assistant specialized on creating concise and accurate summaries on KI topics. Provide a max 20 sentence summary only covering KI topics in German.\"},\n",
        "    {\"role\": \"user\", \"content\": \"You are provided with a transcript of a news Podcast.\" + transcribedAudio},\n",
        "  ]\n",
        ")\n",
        "\n",
        "summary = response.choices[0].message.content\n",
        "display(Markdown(summary))"
      ],
      "metadata": {
        "id": "3D0n_O4PkS_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets use the `generateTextToSpeech` function we implemented before to create an audio with the summary."
      ],
      "metadata": {
        "id": "pe7EtAOnnbfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fileName=generateTextToSpeech(summary)\n",
        "ipd.Audio(filename=fileName)"
      ],
      "metadata": {
        "id": "VDZ10f2unah0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQxMTgU0C9k8"
      },
      "source": [
        "Should German not be your mother tongue, then you can use the `translations` endpoint to transcribe the file into English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U66NNQ1vDLit"
      },
      "outputs": [],
      "source": [
        "def transcribeAndTranslateAudio(fileName):\n",
        "  audio_file = open(fileName, \"rb\")\n",
        "  transcript = client.audio.translations.create(\n",
        "    model=\"whisper-1\",\n",
        "    file=audio_file,\n",
        "    response_format=\"text\"\n",
        "  )\n",
        "  return transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjFKaXsZC8xt"
      },
      "outputs": [],
      "source": [
        "display(Markdown(transcribeAndTranslateAudio(\"Podcast.mp3\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy8C5DdGeqxa"
      },
      "source": [
        "### **Working with Audio Segments**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEvphh0dEAlP"
      },
      "source": [
        "By default, the Whisper API only supports files that are less than 25 MB. To work with larger audio files, you will you will need to break them into chunks.\n",
        "\n",
        "One option is to use the **PyDub open source Python package** to split the audio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0Ef_Lgigs3_"
      },
      "outputs": [],
      "source": [
        "!pip install -q pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkbJu3sVD_Ox"
      },
      "outputs": [],
      "source": [
        "# code adapted from: https://platform.openai.com/docs/guides/speech-to-text/longer-inputs\n",
        "\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def segmentAudio(audioFile):\n",
        "  song = AudioSegment.from_mp3(audioFile)\n",
        "  # PyDub handles time in milliseconds\n",
        "  two_minutes = 2 * 60 * 1000\n",
        "  first_2_minutes = song[:two_minutes]\n",
        "  first_2_minutes.export(\"TwoMinutes\" + audioFile, format=\"mp3\")\n",
        "  return \"TwoMinutes\" + audioFile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will be asked to upload a file to be transcribed.\n",
        "\n",
        "To explore the segmentation capabilities of the Audio API you can experiment with one longer audio files such as https://www.srf.ch/audio/tagesgespraech."
      ],
      "metadata": {
        "id": "t6-3l6s-nNnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O Tagesgespraech_18-01-2025.mp3 \"https://download-media.srf.ch/world/audio/Tagesgespraech_radio/2025/01/Tagesgespraech_radio_AUDI20250118_NR_0030_bf244ffb90af4470abd2cebd91107e91.mp3?d=ap&assetId=641aa6a1-597f-3c50-bce5-a8e0193e76cc\""
      ],
      "metadata": {
        "id": "TLzgTFedv0aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MinVdrf_jCH8"
      },
      "source": [
        "We can then play the split audio file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfvSZQwPf1J7"
      },
      "outputs": [],
      "source": [
        "# The file name which is passed as a parameter to the function transcribeAudio needs to correspond to the file you have uploaded\n",
        "fileName = \"Tagesgespraech_18-01-2025.mp3\"\n",
        "twoMinuteFile = segmentAudio(fileName)\n",
        "ipd.Audio(filename=twoMinuteFile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-yXHgcDjIgu"
      },
      "source": [
        ".. and even translate it to another language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD9freWohDaY"
      },
      "outputs": [],
      "source": [
        "display(Markdown(transcribeAndTranslateAudio(twoMinuteFile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtjqrJXBjza8"
      },
      "source": [
        "To experiment a bit more with Whisper visit [here](https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_processing_guide.ipynb). Moreover, if you want to learn more on prompting with Whisper visit [here](https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_prompting_guide.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUISYB-lmySg"
      },
      "source": [
        "## **Image Generation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5jsCk93n8cL"
      },
      "source": [
        "The Images API provides three methods for interacting with images:\n",
        "\n",
        "* Creating images from scratch based on a text prompt (DALL·E 3 and DALL·E 2)\n",
        "* Creating variations of an existing image (DALL·E 2 only)\n",
        "* Creating edited versions of images by having the model replace some areas of a pre-existing image, based on a new text prompt (DALL·E 2 only)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKCCSklZoFgV"
      },
      "source": [
        "The image generations endpoint allows you to create an original image given a text prompt. When using DALL·E 3, images can have a size of 1024x1024, 1024x1792 or 1792x1024 pixels.\n",
        "\n",
        "By default, images are generated at `standard` quality, but when using DALL·E 3 you can set quality: \"`hd`\" for enhanced detail. Square, standard quality images are the fastest to generate.\n",
        "\n",
        "You can request 1 image at a time with DALL·E 3 (request more by making parallel requests) or up to 10 images at a time using DALL·E 2 with the n parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating Images From Text**"
      ],
      "metadata": {
        "id": "cEjy7ZvDbzZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvUFhEuZm4qS"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.images.generate(\n",
        "  model=\"dall-e-3\",\n",
        "  prompt=\"a downhill skier in the swiss alps\",\n",
        "  size=\"1024x1024\",\n",
        "  quality=\"standard\",\n",
        "  n=1,\n",
        ")\n",
        "\n",
        "image_url = response.data[0].url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R39ftr4xnXfp"
      },
      "outputs": [],
      "source": [
        "# Here we take the URL created previously and open the image\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "\n",
        "with urllib.request.urlopen(image_url) as url:\n",
        "    img=Image.open(url)\n",
        "    display(img)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ApcrD_OGX7Mb",
        "5iqfA8ZvYJBN"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}